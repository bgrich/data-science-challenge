---
title: "Summary"
output: 
  html_notebook:
    toc: true
    toc_float: true
---

The primary question of the Room Key Data Challenge was:

> What factors correlate with users booking and in turn, what can Room Key do 
> to increase booking conversion rate?

The work that I did is contained in the following notebooks: 

* [Data Details](Data_Details.nb.html) 
* [Exploratory Data Analysis](Exploratory_Data_Analysis.nb.html) 
* [Variable Importance via Random Forest](Variable_Importance_RF.nb.html)

In the Data Details notebook, I worked through importing and cleaning the data 
and I did some initial exploration of the data to get a sense of what each 
variable contained. The Exploratory Data Analysis notebook digs deeper into 
each variable in the data set and attempts to either visualize or calculate 
trends in the data. The notebook on Variable Importance via Random Forest 
details an exploration of different ways to train a random forest model to 
determine what variables correlate with users booking. 

The bulk of the work was done in the Exploratory Data Analysis and Variable 
Importance via Random Forest notebooks. This notebook will contain a summary 
of my work, highlighting the key results. For my information on my process and 
all of the steps taken to reach these results, please see the individual 
notebooks.

```{r setup, message = FALSE}
library(tidyverse)
library(jsonlite)
library(caret)

report_theme <- ggplot2::theme_bw() +
  ggplot2::theme(panel.border = ggplot2::element_rect(color = "black",
                                                      fill = NA),
                 panel.grid.major = ggplot2::element_line(color = "black",
                                                          linetype = "dotted"),
                 panel.grid.minor = ggplot2::element_line(color = "black",
                                                          linetype = "dotted"))

lead_ref <- read_csv("lead_ref_data.csv", progress = FALSE)
```

# What variables correlate with booking?

My first step was to examine the data for obvious trends. Though no major 
trends popped out, one fairly useful trend appeared. I created an additional 
variable that gave whether a user left the website to a hotel partner that was 
the same as the referring hotel partner. This appeared to show that a much 
larger proportion of users booked a hotel when they left the site to a hotel 
of that same partner, as opposed to a different partner. 

```{r}
lead_ref %>% 
  mutate(same_partner = (partner_code_referral == rate_partner_lead)) %>% 
  filter(!is.na(same_partner)) %>% 
  ggplot(aes(x = same_partner, fill = booking_event)) +
  geom_bar() +
  coord_flip() +
  ggtitle("Count of Same Partner Events") +
  labs(y = "Count", 
       x = "Same Partner", 
       fill = "Booking Event")
```

```{r}
lead_ref %>% 
  mutate(same_partner = (partner_code_referral == rate_partner_lead)) %>% 
  filter(!is.na(same_partner)) %>% 
  ggplot(aes(x = same_partner, fill = booking_event)) +
  geom_bar(position = "fill") +
  coord_flip() +
  ggtitle("Proportion of Same Partner Events") +
  labs(y = "Proportion", 
       x = "Same Partner", 
       fill = "Booking Event")
```

My next step was to try calculating the correlation between the variables and 
the booking events. I found that for all of the numerical variables, the 
correlation was either incredibly small or non-existant. For the categorical 
variables I used a chi-square test that showed a high significance of 
variability, but gave little information on how those variables influenced 
the booking events.

To determine which variables had the greatest impact on booking, I decided to 
train a random forest model with the data and calculate the importance of the 
different variables to the model.

# Random Forest

I went through many different iterations of the random forest algorithm, trying 
different sets of variables, different metrics for optimizing the 
hyperparameters, and different types of sampling. Many of the initial attempts 
at training a random forest model produced a high accuracy (approximately 
90%), but a close to 0% sensitivity (true positive rate).

Since the quantity 
that we are interested in is the true positive rate (rate of leads converted to 
bookings), I wanted to make some changes. The first major change I made was 
switching the metric for training the hyperparameters from Accuracy (the sum 
of true positives and true negatives divided by the total number of 
observations) to Kappa, which is slightly more robust for an unbalanced 
data set.

When this did not produce a reasonable model, I turned to sampling to solve the 
problem of an unbalanced data set. I tested several different sampling methods 
for the random forest training including over-sampling and under-sampling. I 
also made use of the ROSE and SMOTE methods available from the `caret` package. 
Of these four methods, under-sampling returned the highest sensitivity and 
accuracy for a relatively short computation time. Minimized below is a sample 
of the code used to train the random forest model. In the code shown below are 
also some data modification steps where some of the variables are transformed. 
One of the major transformations done was reducing the number of categories 
for the rate partner variables. I kept the 4 partners with the highest number 
of events and put the other partners into a single group titled "Other". 

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_referral, 
         ts_lead,
         check_in_diff, 
         check_out_diff,
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  mutate(time_diff = ts_lead - ts_referral) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))

rename_referral_partner <- data_for_model %>% 
  group_by(partner_code_referral) %>%
  count() %>%
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_referral_partner = ifelse(rowname < 5, partner_code_referral, "Other")) %>% 
  select(partner_code_referral, new_referral_partner)

rename_lead_partner <- data_for_model %>% 
  group_by(rate_partner_lead) %>%
  count() %>%
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_lead_partner = ifelse(rowname < 5, rate_partner_lead, "Other")) %>% 
  select(rate_partner_lead, new_lead_partner)

modified_data_for_model <- data_for_model %>% 
  left_join(rename_referral_partner, by = "partner_code_referral") %>% 
  left_join(rename_lead_partner, by = "rate_partner_lead") %>% 
  select(-partner_code_referral, -rate_partner_lead) %>% 
  mutate(same_partner = (new_referral_partner == new_lead_partner)) %>% 
  mutate(new_referral_partner = factor(new_referral_partner)) %>% 
  mutate(new_lead_partner = factor(new_lead_partner)) %>% 
  mutate(same_partner = factor(same_partner, levels = c("TRUE", "FALSE")))

set.seed(85)

train_index <- createDataPartition(modified_data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- modified_data_for_model[train_index, ]

testing <- modified_data_for_model[-train_index, ]

train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)

set.seed(85) 

rf_model_under <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "repeatedcv", 
                               number = 10,
                               repeats = 10,
                               sampling = "down"), 
      importance = "permutation", 
      metric = "Kappa")
```

From that random forest model, we can make some predictions using a testing set 
that was separated from the training data set. The results of the predictions 
can be shown as a confusion matrix with some additional summary statistics.

```{r}
rf_prediction_under <- predict(rf_model_under, testing[, -1])

(cm_under <- confusionMatrix(rf_prediction_under, testing$booking_event))
```

This confusion matrix shows the approximately 76% sensitivity with an accuracy 
of about 58%. While this model has a fairly large number of false positives, 
it does pick out some of the important information from the data set. 

# Variable Importance

Now that I have trained a random forest model on the data set, I can extract 
which of the variables were the most important. I tried this using two 
different methods. One is the built in method for the random forest 
implementation in R where the difference in accuracies for the random forest 
model and the model under a permutation of the variables is used. The other 
method I used is the default of the `caret` package where the area under the 
ROC curve is used.

```{r}
varImp(rf_model_under, scale = FALSE)
```

```{r}
varImp(rf_model_under, useModel = FALSE, scale = FALSE)
```

Both of these methods give the `same_partner` and `new_lead_partner` variables 
as the most important. I tested this by training the random forest model again, 
but removing one or both of the variables and comparing the sensitivity and 
accuracy. Removing those variable led to a drop in the sensitivity to 
approximately 62%. I also tested models that were trained with the less 
important variables removed. Those models returned a sensitivity in the 80% 
range, but also led to a decrease in specificity (an increase in the false 
positive rate). 

The results from these models indicate that the `new_lead_partner` variable 
and the `same_partner` variable are the two most important factors in the 
model.

# Recommendations

Given the results of the random forest model training, there are several 
suggestions that I can make. Most of the recommendations are for further 
testing to determine how the `same_partner` and `new_lead_partner` variables 
can be manipulated to produce a higher booking rate. 

First, I would suggest running a test on the `same_partner` variable. This 
test could examine the response of users that only receive hotel suggestions 
that are the same as the referring partner compared to those that receive the 
normal suggestions produced by the website. A similar test could be done 
looking at users that only receive hotel suggestions that are not from the 
referring partner. 

My other recommendation would be to have a discussion with the engineers on 
what type of data can be additionally collected. These models are still 
fairly low on accuracy and additional information on the users could help 
make the models better. 

Of course, this is all dependent on the capabilities of the website and could 
be further refined with discussions with the engineers. 







