---
title: "Variable Importance via Random Forest"
output: 
  html_notebook:
    toc: true
    toc_float: true
---

```{r setup, message = FALSE}
library(tidyverse)
library(jsonlite)
library(caret)

report_theme <- ggplot2::theme_bw() +
  ggplot2::theme(panel.border = ggplot2::element_rect(color = "black",
                                                      fill = NA),
                 panel.grid.major = ggplot2::element_line(color = "black",
                                                          linetype = "dotted"),
                 panel.grid.minor = ggplot2::element_line(color = "black",
                                                          linetype = "dotted"))
```

In this notebook, I plan to use a random forest algorithm to determine the 
importance of the variables in the data set. The variables I will be using 
are: 

* Referral Time (ts_referral) 
* Lead Time (ts_lead) 
* Check-in Difference
* Check-out Difference
* Lead Rate Partner
* Referral Rate Partner
* Same Partner Conditional 
* location-tid
* udicode

# Data Import and Preparation

First, I need to import and prepare the data for analysis. For this, I will 
use the data set created using the `data_manipulation_script.R` file. This data 
set is similar to that used in the Exploratory Data Analysis notebook, with 
some of the extra calculations removed.

```{r, message = FALSE}
lead_ref <- read_csv("lead_ref_data.csv", progress = FALSE)
```

# Random Forest

For this section, I am going to break it into a few subsections where each one 
will be a random forest trained on a different subset of the data. For these 
subsets I am going manipulate the data by filtering out all of the NA terms, 
removing the `udicode` and `location_tid` variables, adding back in those 
variables with some modifications, and finally keeping all of the non-numeric 
NA's.

## NA's and Variables Removed

For this section I am removing the `udicode` and `location_tid` variables 
along with all the remaining NA's.

Here are all of the NA's in the data set.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_lead, 
         ts_referral, 
         check_in_diff, 
         check_out_diff, 
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- data_for_model[train_index, ]

testing <- data_for_model[-train_index, ]
```

### Random Forest Training

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation")
```

```{r}
rf_model
```

### Predictions

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```




First, I want to set up the data for the random forest model. I want to split 
the data into testing and training sets for comparison after running the model. 
I also want to look at the number of NAs present in the data. I plan on 
running the model with a few different variations. Once with all of the data, 
again with the NA's removed, and again with certain variables removed. 

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_lead, 
         ts_referral, 
         check_in_diff, 
         check_out_diff, 
         partner_code_referral, 
         rate_partner_lead, 
         location_tid, 
         udicode)
```

## NAs

Here, I calculate the number of NAs in the data set.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  filter(!is.na(ts_referral)) %>% 
  map_dbl(function(x) sum(is.na(x)))
```

There are about 4500 NAs in the check-in and check-out information. There are 
a smaller number in the ts_referral, location_tid, and partner_code_referral 
variables. There are a large number of NA's (approximate 10k) in the udicode 
variable.

## Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- data_for_model[train_index, ] %>% 
  filter(!is.na(ts_referral), 
         !is.na(partner_code_referral), 
         !is.na(check_in_diff), 
         !is.na(check_out_diff), 
         !is.na(location_tid)) %>% 
  mutate(booking_event = factor(booking_event)) %>% 
  select(-udicode, -location_tid)

testing <- data_for_model[-train_index, ]
```

## Random Forest Training

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "impurity")
```

