---
title: "Variable Importance via Random Forest"
output: 
  html_notebook:
    toc: true
    toc_float: true
---

```{r setup, message = FALSE}
library(tidyverse)
library(jsonlite)
library(caret)

report_theme <- ggplot2::theme_bw() +
  ggplot2::theme(panel.border = ggplot2::element_rect(color = "black",
                                                      fill = NA),
                 panel.grid.major = ggplot2::element_line(color = "black",
                                                          linetype = "dotted"),
                 panel.grid.minor = ggplot2::element_line(color = "black",
                                                          linetype = "dotted"))
```

In this notebook, I plan to use a random forest algorithm to determine the 
importance of the variables in the data set. The variables I will be using 
are: 

* Referral Time (ts_referral) 
* Lead Time (ts_lead) 
* Check-in Difference
* Check-out Difference
* Lead Rate Partner
* Referral Rate Partner
* Same Partner Conditional 
* location-tid
* udicode

# Data Import and Preparation

First, I need to import and prepare the data for analysis. For this, I will 
use the data set created using the `data_manipulation_script.R` file. This data 
set is similar to that used in the Exploratory Data Analysis notebook, with 
some of the extra calculations removed.

```{r, message = FALSE}
lead_ref <- read_csv("lead_ref_data.csv", progress = FALSE)
```

# Random Forest

For this section, I am going to break it into a few subsections where each one 
will be a random forest trained on a different subset of the data. For these 
subsets I am going manipulate the data by filtering out all of the NA terms, 
removing the `udicode` and `location_tid` variables, adding back in those 
variables with some modifications, and finally keeping all of the non-numeric 
NA's.

## NA's and Variables Removed

For this section I am removing the `udicode` and `location_tid` variables 
along with all the remaining NA's.

Here are all of the NA's in the data set.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_lead, 
         ts_referral, 
         check_in_diff, 
         check_out_diff, 
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- data_for_model[train_index, ]

testing <- data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

According to the confusion matrix, while this model does give a high accuracy, 
the sensitivity of the model is incredibly low. Thus, it will be difficult to 
determine what variables are important for booking events.

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

## Using udicode and location_tid

For this section I am retaining the `udicode` and `location_tid` variables. To 
be able to use them in the random forest, I will reduce the number of factors 
to the largest 30 occuring ones. All others will either be lumped into NA or 
Other.

Here are all of the NA's in the data set.


```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_lead, 
         ts_referral, 
         check_in_diff, 
         check_out_diff, 
         partner_code_referral, 
         rate_partner_lead, 
         udicode, 
         location_tid) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```

### Relabeling udicode and location_tid 

```{r}
udicode_rename <- data_for_model %>% 
  group_by(udicode) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_udicode = ifelse(rowname < 30, udicode, "Other")) %>% 
  mutate(new_udicode = ifelse(is.na(new_udicode), "Not Available", new_udicode)) %>% 
 select(udicode, new_udicode)
```

```{r}
location_rename <- data_for_model %>% 
  group_by(location_tid) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_location = ifelse(rowname < 30, location_tid, "Other")) %>% 
  mutate(new_location = ifelse(is.na(location_tid), "Not Available", new_location)) %>% 
  select(location_tid, new_location)
```

```{r}
modified_data_for_model <- data_for_model %>% 
  left_join(udicode_rename, by = "udicode") %>% 
  left_join(location_rename, by = "location_tid") %>% 
  select(-udicode, -location_tid)
```

### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(modified_data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- modified_data_for_model[train_index, ]

testing <- modified_data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

According to the confusion matrix, while this model does give a high accuracy, 
the sensitivity of the model is incredibly low. Thus, it will be difficult to 
determine what variables are important for booking events.

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

## Removing all categorical variables

Here are all of the NA's in the data set.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_lead, 
         ts_referral, 
         check_in_diff, 
         check_out_diff) %>% 
  filter(!is.na(ts_referral)) %>% 
  # filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- data_for_model[train_index, ]

testing <- data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

According to the confusion matrix, while this model does give a high accuracy, 
the sensitivity of the model is incredibly low. Thus, it will be difficult to 
determine what variables are important for booking events.

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

## Removing all numerical variables


For this section I am retaining the `udicode` and `location_tid` variables. To 
be able to use them in the random forest, I will reduce the number of factors 
to the largest 30 occuring ones. All others will either be lumped into NA or 
Other.

Here are all of the NA's in the data set.


```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         partner_code_referral, 
         rate_partner_lead, 
         udicode, 
         location_tid) %>% 
  # filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  # filter(!is.na(check_out_diff)) %>% 
  # filter(!is.na(check_in_diff)) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```

### Relabeling udicode and location_tid 

```{r}
udicode_rename <- data_for_model %>% 
  group_by(udicode) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_udicode = ifelse(rowname < 30, udicode, "Other")) %>% 
  mutate(new_udicode = ifelse(is.na(new_udicode), "Not Available", new_udicode)) %>% 
 select(udicode, new_udicode)
```

```{r}
location_rename <- data_for_model %>% 
  group_by(location_tid) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_location = ifelse(rowname < 30, location_tid, "Other")) %>% 
  mutate(new_location = ifelse(is.na(location_tid), "Not Available", new_location)) %>% 
  select(location_tid, new_location)
```

```{r}
modified_data_for_model <- data_for_model %>% 
  left_join(udicode_rename, by = "udicode") %>% 
  left_join(location_rename, by = "location_tid") %>% 
  select(-udicode, -location_tid)
```

### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(modified_data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- modified_data_for_model[train_index, ]

testing <- modified_data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

According to the confusion matrix, while this model does give a high accuracy, 
the sensitivity of the model is incredibly low. Thus, it will be difficult to 
determine what variables are important for booking events.

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

## Add Same Partner and Time Difference

For this section I am removing the `udicode` and `location_tid` variables 
along with all the remaining NA's.

Here are all of the NA's in the data set.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_lead, 
         ts_referral, 
         check_in_diff, 
         check_out_diff, 
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  mutate(same_partner = (partner_code_referral == rate_partner_lead)) %>% 
  mutate(time_diff = ts_lead - ts_referral) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- data_for_model[train_index, ]

testing <- data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

According to the confusion matrix, while this model does give a high accuracy, 
the sensitivity of the model is incredibly low. Thus, it will be difficult to 
determine what variables are important for booking events.

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

