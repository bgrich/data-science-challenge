---
title: "Variable Importance via Random Forest"
output: 
  html_notebook:
    toc: true
    toc_float: true
---

```{r setup, message = FALSE}
library(tidyverse)
library(jsonlite)
library(caret)

report_theme <- ggplot2::theme_bw() +
  ggplot2::theme(panel.border = ggplot2::element_rect(color = "black",
                                                      fill = NA),
                 panel.grid.major = ggplot2::element_line(color = "black",
                                                          linetype = "dotted"),
                 panel.grid.minor = ggplot2::element_line(color = "black",
                                                          linetype = "dotted"))
```

In this notebook, I plan to use a random forest algorithm to determine the 
importance of the variables in the data set. The variables I will be using 
are: 

* Referral Time (ts_referral) 
* Lead Time (ts_lead) 
* Check-in Difference
* Check-out Difference
* Lead Rate Partner
* Referral Rate Partner
* Same Partner Conditional 
* location-tid
* udicode

# Data Import and Preparation

First, I need to import and prepare the data for analysis. For this, I will 
use the data set created using the `data_manipulation_script.R` file. This data 
set is similar to that used in the Exploratory Data Analysis notebook, with 
some of the extra calculations removed.

```{r, message = FALSE}
lead_ref <- read_csv("lead_ref_data.csv", progress = FALSE)
```

# Random Forest

For this section, I am going to break it into a few subsections where each one 
will be a random forest trained on a different subset of the data. For these 
subsets I am going manipulate the data by filtering out all of the NA terms, 
removing the `udicode` and `location_tid` variables, adding back in those 
variables with some modifications, and finally keeping all of the non-numeric 
NA's.

## NA's and Variables Removed

For this section I am removing the `udicode` and `location_tid` variables 
along with all the remaining NA's.

Here are all of the NA's in the data set.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_lead, 
         ts_referral, 
         check_in_diff, 
         check_out_diff, 
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- data_for_model[train_index, ]

testing <- data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

According to the confusion matrix, while this model does give a high accuracy, 
the sensitivity of the model is incredibly low. Thus, it will be difficult to 
determine what variables are important for booking events.

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

## Using udicode and location_tid

For this section I am retaining the `udicode` and `location_tid` variables. To 
be able to use them in the random forest, I will reduce the number of factors 
to the largest 30 occuring ones. All others will either be lumped into NA or 
Other.

Here are all of the NA's in the data set.


```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_lead, 
         ts_referral, 
         check_in_diff, 
         check_out_diff, 
         partner_code_referral, 
         rate_partner_lead, 
         udicode, 
         location_tid) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```

### Relabeling udicode and location_tid 

```{r}
udicode_rename <- data_for_model %>% 
  group_by(udicode) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_udicode = ifelse(rowname < 30, udicode, "Other")) %>% 
  mutate(new_udicode = ifelse(is.na(new_udicode), "Not Available", new_udicode)) %>% 
 select(udicode, new_udicode)
```

```{r}
location_rename <- data_for_model %>% 
  group_by(location_tid) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_location = ifelse(rowname < 30, location_tid, "Other")) %>% 
  mutate(new_location = ifelse(is.na(location_tid), "Not Available", new_location)) %>% 
  select(location_tid, new_location)
```

```{r}
modified_data_for_model <- data_for_model %>% 
  left_join(udicode_rename, by = "udicode") %>% 
  left_join(location_rename, by = "location_tid") %>% 
  select(-udicode, -location_tid)
```

### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(modified_data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- modified_data_for_model[train_index, ]

testing <- modified_data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

According to the confusion matrix, while this model does give a high accuracy, 
the sensitivity of the model is incredibly low. Thus, it will be difficult to 
determine what variables are important for booking events.

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

## Removing all categorical variables

For this data set I am removing all of the categorical variables from the data 
set.

Here are all of the NA's in the data set.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_lead, 
         ts_referral, 
         check_in_diff, 
         check_out_diff) %>% 
  filter(!is.na(ts_referral)) %>% 
  # filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- data_for_model[train_index, ]

testing <- data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

According to the confusion matrix, while this model does give a high accuracy, 
the sensitivity of the model is incredibly low. Thus, it will be difficult to 
determine what variables are important for booking events.

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

## Removing all numerical variables


For this section I am retaining the `udicode` and `location_tid` variables. I 
am also removing all of the numerical variables. To 
be able to use them in the random forest, I will reduce the number of factors 
to the largest 30 occuring ones. All others will either be lumped into NA or 
Other.

Here are all of the NA's in the data set.


```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         partner_code_referral, 
         rate_partner_lead, 
         udicode, 
         location_tid) %>% 
  # filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  # filter(!is.na(check_out_diff)) %>% 
  # filter(!is.na(check_in_diff)) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```

### Relabeling udicode and location_tid 

```{r}
udicode_rename <- data_for_model %>% 
  group_by(udicode) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_udicode = ifelse(rowname < 30, udicode, "Other")) %>% 
  mutate(new_udicode = ifelse(is.na(new_udicode), "Not Available", new_udicode)) %>% 
 select(udicode, new_udicode)
```

```{r}
location_rename <- data_for_model %>% 
  group_by(location_tid) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_location = ifelse(rowname < 30, location_tid, "Other")) %>% 
  mutate(new_location = ifelse(is.na(location_tid), "Not Available", new_location)) %>% 
  select(location_tid, new_location)
```

```{r}
modified_data_for_model <- data_for_model %>% 
  left_join(udicode_rename, by = "udicode") %>% 
  left_join(location_rename, by = "location_tid") %>% 
  select(-udicode, -location_tid)
```

### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(modified_data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- modified_data_for_model[train_index, ]

testing <- modified_data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

According to the confusion matrix, while this model does give a high accuracy, 
the sensitivity of the model is incredibly low. Thus, it will be difficult to 
determine what variables are important for booking events.

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

## Add Same Partner and Time Difference

For this section I have added two new calculated variables: a conditional 
variable calculating with the referral partner and lead partner are the same 
and the difference in time between referral event and lead event.

Here are all of the NA's in the data set.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_lead, 
         ts_referral, 
         check_in_diff, 
         check_out_diff, 
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  mutate(same_partner = (partner_code_referral == rate_partner_lead)) %>% 
  mutate(time_diff = ts_lead - ts_referral) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- data_for_model[train_index, ]

testing <- data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

According to the confusion matrix, while this model does give a high accuracy, 
the sensitivity of the model is incredibly low. Thus, it will be difficult to 
determine what variables are important for booking events.

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

# Kappa

I realized that accuracy was not working well as a metric for training the 
random forest model. It was leading to too many booking events being classified 
as FALSE while giving a high accuracy. This appears to be due to the unbalanced 
nature of the data. My first step to correct this is to use Kappa as the 
metric for the RF model. Here I reproduce some of the same model trainings as 
previously done, but with the metric switched to Kappa.

## Add Same Partner and Time Difference

For this section I have added two new calculated variables: a conditional 
variable calculating with the referral partner and lead partner are the same 
and the difference in time between referral event and lead event.

Here are all of the NA's in the data set.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_lead, 
         ts_referral, 
         check_in_diff, 
         check_out_diff, 
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  mutate(same_partner = (partner_code_referral == rate_partner_lead)) %>% 
  mutate(time_diff = ts_lead - ts_referral) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- data_for_model[train_index, ]

testing <- data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation", 
      metric = "Kappa")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

According to the confusion matrix, while this model does give a high accuracy, 
the sensitivity of the model is incredibly low. Thus, it will be difficult to 
determine what variables are important for booking events.

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

## NA's and Variables Removed

For this section I am removing the `udicode` and `location_tid` variables 
along with all the remaining NA's.

Here are all of the NA's in the data set.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_lead, 
         ts_referral, 
         check_in_diff, 
         check_out_diff, 
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- data_for_model[train_index, ]

testing <- data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation", 
      metric = "Kappa")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

According to the confusion matrix, while this model does give a high accuracy, 
the sensitivity of the model is incredibly low. Thus, it will be difficult to 
determine what variables are important for booking events.

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

# Removing highly correlated variables 

For this section, I manually removed variables that were highly correlated with 
each other. Particularly, I removed the `ts_lead` and `check_in_out` variables. 

## NA's and Variables Removed

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_referral, 
         check_in_diff, 
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- data_for_model[train_index, ]

testing <- data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation", 
      metric = "Kappa")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

According to the confusion matrix, while this model does give a high accuracy, 
the sensitivity of the model is incredibly low. Thus, it will be difficult to 
determine what variables are important for booking events.

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

## Reducing Factors for Partners

For this section, I reduced the number of factor levels present in the partner 
categorical variables. I did this by taking the top 4 levels and re-labeling 
all other levels as "Other". 

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_referral, 
         check_in_diff, 
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```

### Renaming Partner Codes

```{r}
rename_referral_partner <- data_for_model %>% 
  group_by(partner_code_referral) %>%
  count() %>%
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_referral_partner = ifelse(rowname < 5, partner_code_referral, "Other")) %>% 
  select(partner_code_referral, new_referral_partner)
```

```{r}
rename_lead_partner <- data_for_model %>% 
  group_by(rate_partner_lead) %>%
  count() %>%
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_lead_partner = ifelse(rowname < 5, rate_partner_lead, "Other")) %>% 
  select(rate_partner_lead, new_lead_partner)
```

```{r}
modified_data_for_model <- data_for_model %>% 
  left_join(rename_referral_partner, by = "partner_code_referral") %>% 
  left_join(rename_lead_partner, by = "rate_partner_lead") %>% 
  select(-partner_code_referral, -rate_partner_lead) %>% 
  mutate(same_partner = (new_referral_partner == new_lead_partner))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(modified_data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- modified_data_for_model[train_index, ]

testing <- modified_data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation", 
      metric = "Kappa")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

## Adding in all variables

This data set includes all of the variables (except `udicode` and 
`location_tid`), along with the calculated variables `time_diff` and 
`same_partner`. This data set also includes the reduced factor levels for the 
partner variables.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_referral, 
         ts_lead,
         check_in_diff, 
         check_out_diff,
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  mutate(time_diff = ts_lead - ts_referral) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```

### Renaming Partner Codes

```{r}
rename_referral_partner <- data_for_model %>% 
  group_by(partner_code_referral) %>%
  count() %>%
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_referral_partner = ifelse(rowname < 5, partner_code_referral, "Other")) %>% 
  select(partner_code_referral, new_referral_partner)
```

```{r}
rename_lead_partner <- data_for_model %>% 
  group_by(rate_partner_lead) %>%
  count() %>%
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_lead_partner = ifelse(rowname < 5, rate_partner_lead, "Other")) %>% 
  select(rate_partner_lead, new_lead_partner)
```

```{r}
modified_data_for_model <- data_for_model %>% 
  left_join(rename_referral_partner, by = "partner_code_referral") %>% 
  left_join(rename_lead_partner, by = "rate_partner_lead") %>% 
  select(-partner_code_referral, -rate_partner_lead) %>% 
  mutate(same_partner = (new_referral_partner == new_lead_partner))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(modified_data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- modified_data_for_model[train_index, ]

testing <- modified_data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "cv", 
                               number = 5), 
      importance = "permutation", 
      metric = "Kappa")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

# Under-sampling

To resolve the poor modeling happening with the random forest training, I want 
to use under-sampling and over-sampling to create a more balanced data set for 
the training.

## NA's and Variables Removed, Accuracy, Undersample

For this section I am removing the `udicode` and `location_tid` variables 
along with all the remaining NA's. I will also be under-sampling for training 
and using accuracy as the training metric. I will also use a 10x10 
cross-validation.

Here are all of the NA's in the data set.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_lead, 
         ts_referral, 
         check_in_diff, 
         check_out_diff, 
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- data_for_model[train_index, ]

testing <- data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
set.seed(85)

rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "repeatedcv", 
                               number = 10, 
                               repeats = 10, 
                               sampling = "down"), 
      importance = "permutation")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```





## Adding `time_diff` and `same_partner`, Under-sampled, Accuracy

For this set, I once again have all of the variables except for `udicode` and 
`location_tid`. I also have two calculated variables, `time_diff` and 
`same_partner` that are included. This calculation will also use under-sampling 
and a 10x10 repeated cross-validation. Accuracy is the metric used here for 
training.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_referral, 
         ts_lead,
         check_in_diff, 
         check_out_diff,
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  mutate(time_diff = ts_lead - ts_referral) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```

### Renaming Partner Codes

```{r}
rename_referral_partner <- data_for_model %>% 
  group_by(partner_code_referral) %>%
  count() %>%
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_referral_partner = ifelse(rowname < 5, partner_code_referral, "Other")) %>% 
  select(partner_code_referral, new_referral_partner)
```

```{r}
rename_lead_partner <- data_for_model %>% 
  group_by(rate_partner_lead) %>%
  count() %>%
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_lead_partner = ifelse(rowname < 5, rate_partner_lead, "Other")) %>% 
  select(rate_partner_lead, new_lead_partner)
```

```{r}
modified_data_for_model <- data_for_model %>% 
  left_join(rename_referral_partner, by = "partner_code_referral") %>% 
  left_join(rename_lead_partner, by = "rate_partner_lead") %>% 
  select(-partner_code_referral, -rate_partner_lead) %>% 
  mutate(same_partner = (new_referral_partner == new_lead_partner))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(modified_data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- modified_data_for_model[train_index, ]

testing <- modified_data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
set.seed(85) 

rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "repeatedcv", 
                               number = 10,
                               repeats = 10,
                               sampling = "down"), 
      importance = "permutation")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

## NA's and Variables Removed, Kappa, Undersample

For this section I am removing the `udicode` and `location_tid` variables 
along with all the remaining NA's. I will also be under-sampling for training 
and using Kappa as the training metric. I will also use a 10x10 
cross-validation.

Here are all of the NA's in the data set.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_lead, 
         ts_referral, 
         check_in_diff, 
         check_out_diff, 
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- data_for_model[train_index, ]

testing <- data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
set.seed(85)

rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "repeatedcv", 
                               number = 10, 
                               repeats = 10, 
                               sampling = "down"), 
      importance = "permutation", 
      metric = "Kappa")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```





## Adding `time_diff` and `same_partner`, Under-sampled, Kappa

For this set, I once again have all of the variables except for `udicode` and 
`location_tid`. I also have two calculated variables, `time_diff` and 
`same_partner` that are included. This calculation will also use under-sampling 
and a 10x10 repeated cross-validation. Kappa is the metric used here for 
training.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_referral, 
         ts_lead,
         check_in_diff, 
         check_out_diff,
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  mutate(time_diff = ts_lead - ts_referral) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```

### Renaming Partner Codes

```{r}
rename_referral_partner <- data_for_model %>% 
  group_by(partner_code_referral) %>%
  count() %>%
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_referral_partner = ifelse(rowname < 5, partner_code_referral, "Other")) %>% 
  select(partner_code_referral, new_referral_partner)
```

```{r}
rename_lead_partner <- data_for_model %>% 
  group_by(rate_partner_lead) %>%
  count() %>%
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_lead_partner = ifelse(rowname < 5, rate_partner_lead, "Other")) %>% 
  select(rate_partner_lead, new_lead_partner)
```

```{r}
modified_data_for_model <- data_for_model %>% 
  left_join(rename_referral_partner, by = "partner_code_referral") %>% 
  left_join(rename_lead_partner, by = "rate_partner_lead") %>% 
  select(-partner_code_referral, -rate_partner_lead) %>% 
  mutate(same_partner = (new_referral_partner == new_lead_partner))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(modified_data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- modified_data_for_model[train_index, ]

testing <- modified_data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

```{r}
set.seed(85) 

rf_model <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "repeatedcv", 
                               number = 10,
                               repeats = 10,
                               sampling = "down"), 
      importance = "permutation", 
      metric = "Kappa")
```

```{r}
rf_model
```

### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction <- predict(rf_model, testing[, -1])

confusionMatrix(rf_prediction, testing$booking_event)
```

### Variable Importance

```{r}
varImp(rf_model, scale = FALSE)
```

# Comparing Different Sampling Methods using the full variables and calculated variables

Using under-sampling has appeared to fix the problem with the unbalanced data. 
instead of getting an accuracy of approximately 90% and a sensitivity of close 
to zero, I now get accuracies closer to 60% with sensitvitys of 70% or more. 

I now want to compare under-sampling to other sampling methods used to deal 
with imbalanced data. 

For this set, I once again have all of the variables except for `udicode` and 
`location_tid`. I also have two calculated variables, `time_diff` and 
`same_partner` that are included. This calculation will also use under-sampling 
and a 10x10 repeated cross-validation. Kappa is the metric used here for 
training.

```{r}
lead_ref %>% 
  select(-key, -visitor_id, -booking_event, -check_out_diff, -check_in_diff) %>% 
  # filter(!is.na(ts_referral)) %>%
  map_dbl(function(x) sum(is.na(x)))
```

```{r}
data_for_model <- lead_ref %>% 
  select(booking_event, 
         ts_referral, 
         ts_lead,
         check_in_diff, 
         check_out_diff,
         partner_code_referral, 
         rate_partner_lead) %>% 
  filter(!is.na(ts_referral)) %>% 
  filter(!is.na(partner_code_referral)) %>% 
  filter(!is.na(check_in_diff)) %>% 
  filter(!is.na(check_out_diff)) %>% 
  mutate(time_diff = ts_lead - ts_referral) %>% 
  mutate(booking_event = factor(booking_event, levels = c("TRUE", "FALSE")))
```

### Renaming Partner Codes

```{r}
rename_referral_partner <- data_for_model %>% 
  group_by(partner_code_referral) %>%
  count() %>%
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_referral_partner = ifelse(rowname < 5, partner_code_referral, "Other")) %>% 
  select(partner_code_referral, new_referral_partner)
```

```{r}
rename_lead_partner <- data_for_model %>% 
  group_by(rate_partner_lead) %>%
  count() %>%
  arrange(desc(n)) %>% 
  ungroup() %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  mutate(new_lead_partner = ifelse(rowname < 5, rate_partner_lead, "Other")) %>% 
  select(rate_partner_lead, new_lead_partner)
```

```{r}
modified_data_for_model <- data_for_model %>% 
  left_join(rename_referral_partner, by = "partner_code_referral") %>% 
  left_join(rename_lead_partner, by = "rate_partner_lead") %>% 
  select(-partner_code_referral, -rate_partner_lead) %>% 
  mutate(same_partner = (new_referral_partner == new_lead_partner)) %>% 
  mutate(new_referral_partner = factor(new_referral_partner)) %>% 
  mutate(new_lead_partner = factor(new_lead_partner)) %>% 
  mutate(same_partner = factor(same_partner, levels = c("TRUE", "FALSE")))
```


### Train/Test Split

Now I will randomly split the data into testing and training subsets using an 
80/20 training/testing split. For this, I will set the random seed for R so 
that the results are reproducibile. 

```{r}
set.seed(85)

train_index <- createDataPartition(modified_data_for_model$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- modified_data_for_model[train_index, ]

testing <- modified_data_for_model[-train_index, ]
```

### Random Forest Training

Here I split the training set into the variables and the response for faster 
modeling.

```{r}
train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

Here I train the random forest model using the `ranger` package and with 
a 5-fold cross-validation. I will also use a permutation method to determine 
the variable importance.

### Normal Sampling

```{r}
set.seed(85) 

rf_model_normal <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "repeatedcv", 
                               number = 10,
                               repeats = 10), 
      importance = "permutation", 
      metric = "Kappa")
```

```{r}
rf_model_normal
```

#### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction_normal <- predict(rf_model_normal, testing[, -1])

cm_normal <- confusionMatrix(rf_prediction_normal, testing$booking_event)
```

#### Variable Importance

```{r}
varImp(rf_model_normal, scale = FALSE)
```

### Under Sampling

```{r}
set.seed(85) 

rf_model_under <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "repeatedcv", 
                               number = 10,
                               repeats = 10,
                               sampling = "down"), 
      importance = "permutation", 
      metric = "Kappa")
```

```{r}
rf_model_under
```

#### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction_under <- predict(rf_model_under, testing[, -1])

cm_under <- confusionMatrix(rf_prediction_under, testing$booking_event)
```

#### Variable Importance

```{r}
varImp(rf_model_under, scale = FALSE)
```

### Over Sampling

```{r}
set.seed(85) 

rf_model_over <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "repeatedcv", 
                               number = 10,
                               repeats = 10,
                               sampling = "up"), 
      importance = "permutation", 
      metric = "Kappa")
```

```{r}
rf_model_over
```

#### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction_over <- predict(rf_model_over, testing[, -1])

cm_over <- confusionMatrix(rf_prediction_over, testing$booking_event)
```

#### Variable Importance

```{r}
varImp(rf_model_over, scale = FALSE)
```

### ROSE Sampling

```{r}
set.seed(85) 

rf_model_rose <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "repeatedcv", 
                               number = 10,
                               repeats = 10,
                               sampling = "rose"), 
      importance = "permutation", 
      metric = "Kappa")
```

```{r}
rf_model_rose
```

#### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction_rose <- predict(rf_model_rose, testing[, -1])

cm_rose <- confusionMatrix(rf_prediction_rose, testing$booking_event)
```

#### Variable Importance

```{r}
varImp(rf_model_rose, scale = FALSE)
```

### SMOTE Sampling

```{r}
set.seed(85) 

rf_model_smote <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "repeatedcv", 
                               number = 10,
                               repeats = 10,
                               sampling = "smote"), 
      importance = "permutation", 
      metric = "Kappa")
```

```{r}
rf_model_smote
```

#### Predictions

Here is predict the outcome of the testing data using the random forest model. 
To summarize the model, I build a confusion matrix along with some summary 
statistics.

```{r}
rf_prediction_smote <- predict(rf_model_smote, testing[, -1])

cm_smote <- confusionMatrix(rf_prediction_smote, testing$booking_event)
```

#### Variable Importance

```{r}
varImp(rf_model_smote, scale = FALSE)
```

### Comparison of Results

Here, I take all of the confusion matrix information and put it into a single 
data frame for easy comparison. The confusion matricies with summary statistics 
will be reproduced below.

```{r}
column_names <- c("Sensitivity", "Specificity", "Precision", "Balanced Accuracy")

comparison <- bind_rows(cm_normal$byClass[column_names], 
                        cm_under$byClass[column_names],
                        cm_over$byClass[column_names], 
                        cm_rose$byClass[column_names], 
                        cm_smote$byClass[column_names]) %>% 
  bind_cols(tibble(model = c("Normal", 
                             "Under", 
                             "Over", 
                             "ROSE", 
                             "SMOTE"))) %>% 
  select(model, everything())
```

```{r}
comparison
```

The under-sampled model gives the best sensitivity, though it does have a 
large number of false positives. The model using ROSE for sampling gives a 
similar sensitivity with a slightly btter specificity. However, due to the 
long run time with the ROSE random forest model, I will stick with the slightly 
poorer under-sampled model.

```{r}
cm_normal
```

```{r}
cm_under
```

```{r}
cm_over
```

```{r}
cm_rose
```

```{r}
cm_smote
```

# Variable Importance with Under-Sampled RF Model

Now I want to look at a few different measure of variable importance to finally 
answer the question of which factor correlates the most the booking events.

```{r}
varImp(rf_model_under, scale = FALSE)
```

This first measure compares the accuracy of the original model with the 
accuracy of the model when the variables are permuted. It appears that the 
`same_partner` and `new_lead_partner` variables are the most important with 
the `time_diff` variable being the least important.

```{r}
varImp(rf_model_under, useModel = FALSE, scale = FALSE)
```

The second measure examines the area under the ROC curve. Here we again have 
the `same_partner` and `new_lead_partner` variables as the most important with 
the `time_diff` variable now near the top.

## Removing Low Importance Variables

I'm now going to remove some of the low importance variables and see if we can 
get a better trained model.

### Keeping `same_partner`, `new_lead_partner` only

```{r}
var_remove <- modified_data_for_model %>% 
  select(booking_event, same_partner, new_lead_partner)

set.seed(85)

train_index <- createDataPartition(var_remove$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- var_remove[train_index, ]

testing <- var_remove[-train_index, ]

train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

```{r}
set.seed(85) 

var_remove_model_1 <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "repeatedcv", 
                               number = 10,
                               repeats = 10,
                               sampling = "down"), 
      importance = "permutation", 
      metric = "Kappa")
```

```{r}
var_remove_model_1
```

```{r}
rf_prediction_var_remove_1 <- predict(var_remove_model_1, testing[, -1])

cm_var_remove_1 <- confusionMatrix(rf_prediction_var_remove_1, testing$booking_event)
```

### Keeping `same_partner`, `new_lead_partner`, `new_referral_partner`, `check_out_diff`

```{r}
var_remove <- modified_data_for_model %>% 
  select(booking_event, same_partner, new_lead_partner, new_referral_partner, check_out_diff)

set.seed(85)

train_index <- createDataPartition(var_remove$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- var_remove[train_index, ]

testing <- var_remove[-train_index, ]

train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

```{r}
set.seed(85) 

var_remove_model_2 <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "repeatedcv", 
                               number = 10,
                               repeats = 10,
                               sampling = "down"), 
      importance = "permutation", 
      metric = "Kappa")
```

```{r}
var_remove_model_2
```

```{r}
rf_prediction_var_remove_2 <- predict(var_remove_model_2, testing[, -1])

cm_var_remove_2 <- confusionMatrix(rf_prediction_var_remove_2, testing$booking_event)
```

### Keeping `same_partner`, `new_lead_partner`, and `time_diff`

```{r}
var_remove <- modified_data_for_model %>% 
  select(booking_event, same_partner, new_lead_partner, time_diff)

set.seed(85)

train_index <- createDataPartition(var_remove$booking_event, 
                            p = 0.8,
                            list = FALSE, 
                            times = 1)

training <- var_remove[train_index, ]

testing <- var_remove[-train_index, ]

train_independent <- training %>% 
  select(-booking_event) %>% 
  as.data.frame()

train_dependent <- training %>% 
  select(booking_event)
```

```{r}
set.seed(85) 

var_remove_model_3 <- train(x = train_independent, 
                  y = train_dependent$booking_event, 
      method = "ranger", 
      trControl = trainControl(method = "repeatedcv", 
                               number = 10,
                               repeats = 10,
                               sampling = "down"), 
      importance = "permutation", 
      metric = "Kappa")
```

```{r}
var_remove_model_3
```

```{r}
rf_prediction_var_remove_3 <- predict(var_remove_model_3, testing[, -1])

cm_var_remove_3 <- confusionMatrix(rf_prediction_var_remove_3, testing$booking_event)
```

## Comparing Confusion Matricies

```{r}
column_names <- c("Sensitivity", "Specificity", "Precision", "Balanced Accuracy")

comparison_var_remove <- bind_rows(cm_under$byClass[column_names], 
                        cm_var_remove_1$byClass[column_names],
                        cm_var_remove_2$byClass[column_names], 
                        cm_var_remove_3$byClass[column_names]) %>% 
  bind_cols(tibble(model = c("Under", 
                             "Var Remove 1", 
                             "Var Remove 2", 
                             "Var Remove 3"))) %>% 
  select(model, everything())
```

```{r}
comparison_var_remove
```

```{r}
cm_under
```

```{r}
cm_var_remove_1
```

```{r}
cm_var_remove_2
```

```{r}
cm_var_remove_3
```

